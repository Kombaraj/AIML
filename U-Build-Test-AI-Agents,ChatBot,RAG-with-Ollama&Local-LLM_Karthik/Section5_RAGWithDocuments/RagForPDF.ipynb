{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAG with PDF ðŸ“„ Data extraction to give context to LLM ðŸ§ \n",
    "\n",
    "<img src=\"./Images/RAGs.png\" width=\"800\" height=\"700\" style=\"display: block; margin: auto;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pypdf\n",
      "  Downloading pypdf-5.4.0-py3-none-any.whl.metadata (7.3 kB)\n",
      "Downloading pypdf-5.4.0-py3-none-any.whl (302 kB)\n",
      "Installing collected packages: pypdf\n",
      "Successfully installed pypdf-5.4.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load = load_dotenv('./../.env')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "llm = ChatOllama(\n",
    "    base_url=\"http://localhost:11434\",\n",
    "    model = \"qwen3:1.7b\",\n",
    "    temperature=0.5,\n",
    "    max_tokens = 250\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Extracting the PDF files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "pdf1 = \"./attention.pdf\"\n",
    "pdf2 = \"./LLMForgetting.pdf\"\n",
    "pdf3 = \"./TestingAndEvaluatingLLM.pdf\"\n",
    "\n",
    "pdfFiles = [pdf1, pdf2, pdf3]\n",
    "\n",
    "documents = []\n",
    "\n",
    "for pdf in pdfFiles:\n",
    "    loader = PyPDFLoader(pdf)\n",
    "    documents.extend(loader.load())\n",
    "\n",
    "print(len(documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(documents[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Text Splitting into Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, \n",
    "                                               chunk_overlap=200, add_start_index=True)\n",
    "\n",
    "all_splits = text_splitter.split_documents(documents)\n",
    "\n",
    "len(all_splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "embeddings = OllamaEmbeddings(model=\"llama3.2:latest\")\n",
    "\n",
    "vector_1 = embeddings.embed_query(all_splits[0].page_content)\n",
    "vector_2 = embeddings.embed_query(all_splits[1].page_content)\n",
    "\n",
    "assert len(vector_1) == len(vector_2)\n",
    "len(vector_1), len(vector_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Vector Stores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -qU langchain-chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "\n",
    "vector_store = Chroma.from_documents(\n",
    "    documents = all_splits,\n",
    "    embedding=embeddings,\n",
    "    persist_directory=\"./chroma_langchain_db\",  # Where to save data locally, remove if not necessary\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Retriving from the Persistant Vector Datastore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "\n",
    "\n",
    "vector_store = Chroma(persist_directory='./chroma_langchain_db', embedding_function=embeddings)\n",
    "\n",
    "result = vector_store.similarity_search(\"What is Bias testing\", k=3)\n",
    "\n",
    "for doc in result:\n",
    "    print(doc.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = vector_store.similarity_search_with_score(\"What are the types of LLM Testing\")\n",
    "\n",
    "result[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Retrivers in Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_store.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs = {\"k\": 1}\n",
    ")\n",
    "\n",
    "retriever.batch(\n",
    "    [\n",
    "        \"What is the Bias Measurement\",\n",
    "        \"How to test human safety against LLM\",\n",
    "        \"How LLM forgets the context\"\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document Retrival Manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "query = \"What exactly does Testing the Factual Correctness of LLM tells\"\n",
    "\n",
    "retrieved_docs = retriever.get_relevant_documents(query)\n",
    "\n",
    "context_text = \"\\n\\n\".join([doc.page_content for doc in retrieved_docs])\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    You are an AI Assisant. Use the following context to answer the question correctly.\n",
    "    If you dont know the answer, just tell, I dont know.\n",
    "    \n",
    "    Also, summarize the response in MD format\n",
    "    \n",
    "    \"context: {context} \\n\\n\"\n",
    "    \"question: {question} \\n\\n\"\n",
    "    \"AI answer:\n",
    "    \n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "chain = prompt_template | llm | StrOutputParser()\n",
    "\n",
    "response = chain.invoke({\"context\": context_text, \"question\": query})\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Langchain Hub for Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain import hub\n",
    "\n",
    "query = \"How to test Translation in LLM?\"\n",
    "\n",
    "retrieved_docs = retriever.get_relevant_documents(query)\n",
    "\n",
    "context_text = \"\\n\\n\".join([doc.page_content for doc in retrieved_docs])\n",
    "\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "response = chain.invoke({\"context\": context_text, \"question\": query})\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieving data using RetrievalQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(llm, retriever=retriever, return_source_documents=True)\n",
    "\n",
    "question = \"What exactly does Testing the Factual Correctness of LLM tells\"\n",
    "\n",
    "response = qa_chain.invoke(question)\n",
    "\n",
    "sources = set(doc.metadata.get(\"source\", \"Unknown\") for doc in response[\"source_documents\"])\n",
    "\n",
    "print(response['result'])\n",
    "print(\"\\nðŸ“• Sources Used:\")\n",
    "for source in sources:\n",
    "    print(f\"- {source}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
